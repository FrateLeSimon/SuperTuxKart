## Imitation Learning for Autonomous Agents in SuperTuxKart Ice Hockey

This study explores imitation learning in the context of the video game SuperTuxKart, focusing specifically on its "ice hockey" game mode. The objective is to train an autonomous agent capable of reproducing the behavior of an expert agent, without relying on human demonstrations. The approach is based on supervised learning from trajectories generated by a high-performing built-in AI agent, selected from among several available in the game.

A dataset is constructed using matches in which the expert agent, named jurgen, successfully scored goals. Game states (relative positions, velocities, angles) are transformed into feature vectors and paired with the expertâ€™s corresponding actions (acceleration, steering, braking). A fully connected neural network is then trained to imitate these actions based on the observed states.

The DAgger method is also evaluated: it involves letting the imitation agent act autonomously while querying the expert to correct its decisions, thereby enriching the dataset with out-of-distribution situations. However, in this context, DAgger proves to be less effective than direct imitation learning, primarily due to a lack of diversity in the encountered scenarios.

Evaluation of the trained policies shows that the best performance is achieved when two independently trained imitation agents play together. This behavioral diversity promotes complementarity and stronger collective performance. In contrast, agents trained with DAgger yield poor results, indicating limitations in exploration and generalization.

This study demonstrates that supervised learning from artificial expert agents can produce credible behavior in a dynamic, multi-agent game environment. It also highlights the specific challenges associated with using DAgger in environments where corrective data may be redundant or insufficiently informative.